---
title: Time Series Clustering COVID-19 Cases
author: Conor Tompkins
date: '2020-07-20'
slug: time-series-clustering-covid-19-cases
categories:
  - R
tags:
  - COVID
---

This post will be about clustering COVID-19 case data. The goal is to group states into clusters based on the state's cumulative sum of COVID-19 cases.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

Load the packages I will use in the analysis and set up the environment:
```{r}
library(tidyverse)
library(tsibble)
library(dtwclust)
library(tidymodels)
library(hrbrthemes)
library(tidycensus)
library(sf)

options(scipen = 999, digits = 4)

theme_set(theme_ipsum())

set.seed(1234)
```


I will adjust the cases to per 100,000, which requires information from the U.S. Census. This code pulls state-level population data from the Census API via `tidycensus`:
```{r}
census_data <- get_acs(geography = "state", variables = "B01003_001", geometry = FALSE) %>% 
  select(state = NAME, population = estimate)
```


This pulls the COVID-19 data from the [NYTimes GitHub page]():
```{r}
covid <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv") %>% 
  arrange(state, date) %>% 
  semi_join(census_data)
```

I use the `tsibble` package to check if there are implicit gaps in the data. For example, if there was data for 2020-06-01 and 2020-06-03, there is an implicit gap because there is not data for 2020-06-02.
```{r}
covid %>% 
  as_tsibble(index = date, key = state) %>% 
  count_gaps()
```

Thankfully, there are not any such gaps. If  there were, I would have to impute values for the missing days.

Since states experienced onset of COVID-19 at different times, I find the day each state hit 10 cases, and calculate `days_since_10th_case`, which I will use instead of `date`.
```{r}
covid_10th_case <- covid %>% 
  filter(cases >= 10) %>% 
  group_by(state) %>% 
  slice(1) %>% 
  ungroup() %>% 
  select(state, date_of_10th_case = date)

covid <- covid %>% 
  left_join(covid_10th_case, by = c("state" = "state")) %>% 
  group_by(state) %>% 
  mutate(days_since_10th_case = date - date_of_10th_case) %>% 
  ungroup() %>% 
  filter(days_since_10th_case >= 0)

covid <- covid %>% 
  select(state, days_since_10th_case, cases)
```

Next I calculate `cases_per_capita`:
```{r}
covid <- covid %>% 
  left_join(census_data) %>% 
  mutate(cases_per_capita = (cases / population) * 100000) %>% 
  select(-population)
```

Next I scale the cases so that the mean is 0 and the standard deviation is 1. Each state has its own mean and standard deviation.
```{r}
covid <- covid %>% 
  group_by(state) %>% 
  mutate(cases_per_capita_scaled = scale(cases_per_capita, center = TRUE, scale = TRUE)) %>% 
  ungroup()
```

The result of this is that the clustering algorithm will focus on the *shape* of the line for each state instead of absolute values. This graph shows the difference:
```{r}
covid %>% 
  pivot_longer(cols = contains("cases"), names_to = "metric", values_to = "value") %>% 
  ggplot(aes(days_since_10th_case, value, group = state)) +
  geom_line(alpha = .4) +
  geom_hline(yintercept = 0, linetype = 2) +
  facet_wrap(~metric, ncol = 1, scales = "free_y")
```
`tsclust` requires that the input data be a series of lists, not a dataframe. `unstack` takes a `key` and `value` as arguments and turns the dataframe into a list of lists.
```{r}
covid_list <- covid %>% 
  select(state, cases_per_capita_scaled) %>% 
  unstack(cases_per_capita_scaled ~ state)
```

This loops through the clustering function 20 times and saves each output to a list. The first object groups the data into 2 clusters, the second object has 3 clusters, and it continues in that pattern.
```{r eval = FALSE}
cluster_dtw_h <- list()

kclust <- 20

for (i in 2:kclust){
  cluster_dtw_h[[i]] <- tsclust(covid_list, 
                                type = "h", 
                                k = i,
                                distance = "dtw", 
                                control = hierarchical_control(method = "complete"), 
                                seed = 390, 
                                preproc = NULL, 
                                args = tsclust_args(dist = list(window.size = 21L)))
  
  print(i)
}
```

The object that `tsclust` outputs has a complex structure that makes it difficult to work with at scale. The data I need to pull out is stored in various slots:
```{r}
slotNames(cluster_dtw_h[[2]])
```
```{r}
str(cluster_dtw_h[[2]])
```

The next step is to write functions that pulls out the data and tidies it up.

This function pulls which cluster each state was assigned to, for each `kclust`.
```{r eval = FALSE}
get_cluster_assignments <- function(object, cluster_number){
  
  df <- slot(object[[cluster_number]], "cluster")

  return(df)
}
```

```{r eval = FALSE}
cluster_assignments <- 2:kclust %>%
  set_names() %>% 
  map_df(~get_cluster_assignments(cluster_dtw_h, cluster_number = .x), .id = "kclust") %>% 
  pivot_longer(cols = -kclust, names_to = "state", values_to = "cluster_assignment") %>% 
  mutate(kclust = as.numeric(kclust)) %>% 
  arrange(state, kclust)
```
```{r include=FALSE}
cluster_assignments <- read_csv("")
```
```{r}
glimpse(cluster_assignments)
```

These graphs shows which states are more likely to be assigned to a different cluster, depending on the number of clusters.
```{r}
state_variance <- cluster_assignments %>% 
  distinct(state, cluster_assignment) %>% 
  count(state, sort = TRUE)

cluster_assignments %>%
  left_join(state_variance) %>% 
  mutate(state = fct_reorder(state, n)) %>% 
  ggplot(aes(kclust, state, fill = as.factor(cluster_assignment))) +
  geom_tile() +
  scale_fill_viridis_d()
```
```{r}
cluster_assignments %>% 
  distinct(state, cluster_assignment) %>% 
  count(state) %>% 
  mutate(state = fct_reorder(state, n)) %>% 
  ggplot(aes(n, state)) +
  geom_col() +
  labs(title = "How much does each state react to a change in kclust?")
```

The number of singelton clusters (clusters with only one state) is an important metric for determining the optimal number of clusters. If a state is truly unique, a singleton cluster may be appropriate. Having 50 singleton clusters, however, would obviously be overfit.

There is not a singleton cluster until `kclust` is 6.
```{r}
cluster_assignments %>% 
  count(kclust, cluster_assignment) %>% 
  group_by(kclust) %>% 
  mutate(min_cluster_population = min(n)) %>% 
  ungroup() %>% 
  #filter(min_clusters > 2) %>% 
  count(kclust, min_cluster_population) %>% 
  mutate(first_singleton = cumsum(min_cluster_population == 1) == 1) %>%
  filter(first_singleton == TRUE)
```
```{r eval = FALSE, include = FALSE}
cluster_assignments %>% 
  count(kclust, cluster_assignment) %>% 
  ggplot(aes(kclust, n, color = as.factor(cluster_assignment))) +
  geom_jitter(show.legend = FALSE)
```


This function pulls the average distance of each cluster, for each value of `kclust`. Clusters with lower average distance are more similar, and those with higher average distance are less similar.
```{r eval = FALSE}
get_cluster_metrics <- function(object, cluster_number){
  
  df <- slot(object[[cluster_number]], "clusinfo")
  
  return(df)
}
```
```{r include = FALSE}
cluster_metrics <- read_csv("")
```

This shows that 11 clusters may be the optimal number for `kclust`. Values greater than that begin to see diminishing returns.
```{r}
cluster_metrics %>% 
  ggplot(aes(kclust, av_dist)) +
  geom_jitter(aes(color = as.factor(kclust), size = size), show.legend = FALSE) +
  geom_smooth(group = 1) +
  geom_vline(xintercept = 11, linetype = 2) +
  scale_size_continuous(range = c(.5, 4))
```

This shows what the individual state time series data looks like when it is grouped into 11 clusters:
```{r}
best_kclust <- 8

cluster_assignments %>% 
  filter(kclust == best_kclust)
```
```{r}
covid %>% 
  as_tibble() %>% 
  left_join(filter(cluster_assignments, kclust == best_kclust)) %>% 
  add_count(cluster_assignment) %>% 
  mutate(cluster_assignment = str_c("Cluster", cluster_assignment, sep = " "),
         cluster_assignment = fct_reorder(as.character(cluster_assignment), n),
         cluster_assignment = fct_rev(cluster_assignment)) %>% 
  ggplot(aes(days_since_10th_case, cases_per_capita_scaled, 
             color = cluster_assignment, group = state)) +
  geom_line(alpha = .3) +
  #geom_line(data = filter(covid, state == "Alaska") %>% 
  #            left_join(filter(cluster_assignments, kclust == best_kclust)), 
  #          size = 1, color = "black") +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_vline(xintercept = 60, linetype = 2) +
  facet_wrap(~cluster_assignment, ncol = 4) +
  guides(color = FALSE)
```

I think 11 clusters is appropriate, but there are obviously cases where individual assignments can be disputed. This is a supervised clustering problem, so I generally pick the `kclust` with the least diminishing returns and go with it.

## Mapping

The data is aggregated at the state level, which can easily be graphed with `ggplot2` and `tidycensus`.
```{r}
map <- get_acs(geography = "state", variables = "B01003_001", geometry = TRUE, shift_geo = TRUE)

map %>% 
  ggplot() +
  geom_sf()
```

This joins the cluster assignments to the map object and summarizes the state polygons by region. This dissolves the state boundaries and creates polygons for each cluster.
```{r}
map_cluster <- map %>% 
  left_join(cluster_assignments %>% 
             filter(kclust == best_kclust), by = c("NAME" = "state")) %>% 
  add_count(cluster_assignment) %>% 
  mutate(cluster_assignment = as.character(cluster_assignment),
         cluster_assignment = fct_reorder(cluster_assignment, desc(n))) %>% 
  group_by(cluster_assignment) %>% 
  summarize()

state_clustered <- map %>% 
  left_join(cluster_assignments %>% 
              filter(kclust == best_kclust), by = c("NAME" = "state")) %>% 
  add_count(cluster_assignment) %>% 
  mutate(cluster_assignment = as.character(cluster_assignment),
         cluster_assignment = fct_reorder(cluster_assignment, desc(n)))
```

This code creates the map, and overlays the state boundaries on the cluster polygons.
```{r}
map_cluster %>% 
  ggplot() +
  geom_sf(aes(fill = cluster_assignment),
          size = 1) +
  geom_sf(data = map, color = "grey", size = .1, alpha = 0) +
  geom_sf_label(data = state_clustered, 
                aes(label = cluster_assignment, color = cluster_assignment), 
                alpha = .8,
                size = 3,
                show.legend = FALSE)
```



```{r}
cluster_assignments %>% 
  filter(kclust == best_kclust) %>% 
  count(cluster_assignment, sort = TRUE) %>% 
  mutate(cluster_assignment = as.character(cluster_assignment),
         cluster_assignment = fct_reorder(cluster_assignment, n)) %>% 
  ggplot(aes(n, cluster_assignment, fill = cluster_assignment)) +
  geom_col(color = "black", show.legend = FALSE)
```









### Sources
https://rpubs.com/esobolewska/dtw-time-series
http://www.rdatamining.com/examples/time-series-clustering-classification
http://rstudio-pubs-static.s3.amazonaws.com/398402_abe1a0343a4e4e03977de8f3791e96bb.html